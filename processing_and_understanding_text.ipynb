{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Extracting text data by using web scraping\nimport requests\nfrom bs4 import BeautifulSoup\n\nheaders = {\n    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n    AppleWebKit/537.36(KHTML, like Gecko) Chrome/110.0.5481.178 Safari/537.36\"\n}\nurl=\"https://www.hsph.harvard.edu/nutritionsource/what-should-you-eat/whole-grains/\"\n\npage = requests.get(url, headers=headers)\nsoup = BeautifulSoup(page.content, \"html.parser\")\ntitle = soup.title.get_text().strip()\n\nparagraphs=[]\nfor para in soup.find_all(\"p\"):\n    paragraph = para.get_text()\n    paragraphs += [paragraph]\n    \nbody = str(\"\\n\\n\".join(paragraphs))\ntext = title + \"\\n\\n\" + body","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:10.567006Z","iopub.execute_input":"2023-05-19T15:30:10.567332Z","iopub.status.idle":"2023-05-19T15:30:10.680841Z","shell.execute_reply.started":"2023-05-19T15:30:10.567309Z","shell.execute_reply":"2023-05-19T15:30:10.679880Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Splitting text into sentences and words, pos tagging using NLTK\nimport nltk\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\n\nsent_list = sent_tokenize(text)\nword_list = word_tokenize(text)\npos = pos_tag(word_list)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:10.682421Z","iopub.execute_input":"2023-05-19T15:30:10.682760Z","iopub.status.idle":"2023-05-19T15:30:10.764195Z","shell.execute_reply.started":"2023-05-19T15:30:10.682730Z","shell.execute_reply":"2023-05-19T15:30:10.762995Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Stemming\nfrom nltk.stem import SnowballStemmer\nss = SnowballStemmer(\"english\")\nstem = [ss.stem(word) for word in word_list]","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:10.766779Z","iopub.execute_input":"2023-05-19T15:30:10.767147Z","iopub.status.idle":"2023-05-19T15:30:10.786801Z","shell.execute_reply.started":"2023-05-19T15:30:10.767119Z","shell.execute_reply":"2023-05-19T15:30:10.785086Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import wordnet as wn\nnltk.download('omw-1.4')\n#nltk.download('wordnet2022')\n\n#! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:10.789033Z","iopub.execute_input":"2023-05-19T15:30:10.789752Z","iopub.status.idle":"2023-05-19T15:30:10.808858Z","shell.execute_reply.started":"2023-05-19T15:30:10.789722Z","shell.execute_reply":"2023-05-19T15:30:10.807779Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nwn = WordNetLemmatizer()\nlem = wn.lemmatize(\"experiences\")","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:10.810421Z","iopub.execute_input":"2023-05-19T15:30:10.810780Z","iopub.status.idle":"2023-05-19T15:30:10.819412Z","shell.execute_reply.started":"2023-05-19T15:30:10.810745Z","shell.execute_reply":"2023-05-19T15:30:10.818480Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Contraction\n!pip install contractions\nimport contractions\nsentence = \"She didn't tell me the story\"\ncon = contractions.fix(sentence)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:10.820950Z","iopub.execute_input":"2023-05-19T15:30:10.821305Z","iopub.status.idle":"2023-05-19T15:30:20.153829Z","shell.execute_reply.started":"2023-05-19T15:30:10.821273Z","shell.execute_reply":"2023-05-19T15:30:20.152348Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Requirement already satisfied: contractions in /opt/conda/lib/python3.10/site-packages (0.1.73)\nRequirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.10/site-packages (from contractions) (0.0.24)\nRequirement already satisfied: anyascii in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\nRequirement already satisfied: pyahocorasick in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Correct spelling\n!pip install autocorrect\nfrom autocorrect import Speller\nspell = Speller()\ntext = \"hte car is luxury\"\ncorr = spell(text)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:20.155323Z","iopub.execute_input":"2023-05-19T15:30:20.155700Z","iopub.status.idle":"2023-05-19T15:30:29.331582Z","shell.execute_reply.started":"2023-05-19T15:30:20.155664Z","shell.execute_reply":"2023-05-19T15:30:29.330158Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Requirement already satisfied: autocorrect in /opt/conda/lib/python3.10/site-packages (2.6.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Remove stopwords\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\nremove_stopwords = []\nfor word in word_list:\n    if word not in stop_words:\n        remove_stopwords.append(word)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:30:29.332965Z","iopub.execute_input":"2023-05-19T15:30:29.333276Z","iopub.status.idle":"2023-05-19T15:30:29.339931Z","shell.execute_reply.started":"2023-05-19T15:30:29.333252Z","shell.execute_reply":"2023-05-19T15:30:29.338675Z"},"trusted":true},"execution_count":60,"outputs":[]}]}